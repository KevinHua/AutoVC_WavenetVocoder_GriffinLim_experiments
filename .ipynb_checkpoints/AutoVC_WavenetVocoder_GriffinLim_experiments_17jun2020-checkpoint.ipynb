{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AutoVC [https://arxiv.org/abs/1905.05879] achieves zero-shot voice conversion using only a vanilla auto-encoder loss. The authors provide in their repo [https://github.com/auspicious3000/autovc] a pretrained model and the speaker embeddings of 4 of their speakers. They use a wavenet vocoder [https://github.com/r9y9/wavenet_vocoder] to convert spectrograms to audio. \n",
    "\n",
    "This repo build upon the steps in [https://github.com/miaoYuanyuan/gen_melSpec_from_wav], who expressed the steps of spectrogram preprocessing more clearly.\n",
    "\n",
    "AutoVC performs Voice Conversion in three steps;\n",
    "- Convert a mel-spectrogram representation of a person's speech into a mel-spectrogram representation of another person's speech (AutoVC)\n",
    "- Refine the mel-spectrogram (PostNet)\n",
    "- Convert a mel-spectrogram to a waveform representation (Wavenet Vocoder)\n",
    "\n",
    "In this notebook we'll break those two steps down. We'll invert a mel-spectrogram obtained by AutoVC (with and without PostNet) with the classic Griffin-Lim algorith, and we'll invert a mel-spectrogram of real audio using the WaveNet vocoder.\n",
    "\n",
    "The classic Griffin-Lim algorithm requires linear-scaled spectrograms (instead of mel-scaled spectrograms), therefore, we must first convert our mel spectrograms to linear scale spectrograms. As we shall see, some information (and therefore quality) gets lost in the the mel-to-linear transformation. \n",
    "\n",
    "```\n",
    "Experiments to run (normalization steps omitted for brevity):\n",
    "\n",
    "VC = AutoVC (requires mel spectrogram)\n",
    "PN = PostNet (requires mel spectrogram)\n",
    "WV = Wavenet Vocoder spectrogram inversion (requires mel spectrogram)\n",
    "GL = Griffin-Lim spectrogram inversion (requires linear spectrogram)\n",
    "\n",
    "A. aud_in --> lin_in --> mel_in -->                                                   --> WV --> aud_out \n",
    "B. aud_in --> lin_in -->                                                              --> GL --> aud_out \n",
    "C. aud_in --> lin_in --> mel_in -->                                       --> lin_out --> GL --> aud_out \n",
    "\n",
    "D. aud_in --> lin_in --> mel_in --> VC --> mel_out_1 --> PN --> mel_out_2 -->         --> WV --> aud_out\n",
    "E. aud_in --> lin_in --> mel_in --> VC --> mel_out_1 -->                  -->         --> WV --> aud_out\n",
    "F. aud_in --> lin_in --> mel_in --> VC --> mel_out_1 --> PN --> mel_out_2 --> lin_out --> GL --> aud_out\n",
    "G. aud_in --> lin_in --> mel_in --> VC --> mel_out_1 -->                  --> lin_out --> GL --> aud_out\n",
    "\n",
    "Naming:\n",
    "<spkr_in>_A_lin_mel_WV.wav\n",
    "<spkr_in>_B_lin_GL.wav\n",
    "<spkr_in>_C_lin_mel_lin_GL.wav\n",
    "<spkr_in>_D_<spkr_out>_lin_mel_VC_mel_PN_mel_WV.wav\n",
    "<spkr_in>_E_<spkr_out>_lin_mel_VC_mel_WV.wav\n",
    "<spkr_in>_F_<spkr_out>_lin_mel_VC_mel_PN_mel_lin_GL.wav\n",
    "<spkr_in>_G_<spkr_out>_lin_mel_VC_mel_lin_GL.wav\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/auspicious3000/autovc\n",
    "# https://github.com/r9y9/wavenet_vocoder/\n",
    "# https://github.com/miaoYuanyuan/gen_melSpec_from_wav\n",
    "\n",
    "# NOTE: for the pretrained wavenet vocoder at https://github.com/auspicious3000/autovc:\n",
    "#       hparams file says fmin=125,        but actually fmin=90\n",
    "#       hparams file says ref_level_db=20, but actually ref_level_db=16\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import librosa\n",
    "\n",
    "import IPython.display as ipd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified from https://github.com/auspicious3000/autovc\n",
    "from synthesis import build_model\n",
    "from synthesis import wavegen\n",
    "from hparams import hparams\n",
    "\n",
    "wavenet_vocoder = build_model().to(device)\n",
    "checkpoint = torch.load(\"checkpoint_step001000000_ema.pth\", map_location=device)\n",
    "wavenet_vocoder.load_state_dict(checkpoint[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified from https://github.com/auspicious3000/autovc\n",
    "from model_vc import Generator\n",
    "\n",
    "def pad_seq(x, base=32):\n",
    "    len_out = int(base * math.ceil(float(x.shape[0])/base))\n",
    "    len_pad = len_out - x.shape[0]\n",
    "    assert len_pad >= 0\n",
    "    return np.pad(x, ((0,len_pad),(0,0)), 'constant'), len_pad\n",
    "\n",
    "auto_VC = Generator(32,256,512,32).eval().to(device)\n",
    "g_checkpoint = torch.load('autovc.ckpt', map_location=device) \n",
    "auto_VC.load_state_dict(g_checkpoint['model'])\n",
    "\n",
    "metadata = pickle.load(open('metadata.pkl', \"rb\"))\n",
    "spkr_to_embed = {entry[0] : entry[1] for entry in metadata}\n",
    "\n",
    "def apply_autoVC(mel, embed_in, embed_out):\n",
    "    # assume normalized mel spectrogram as input (normalized to db scale)\n",
    "    # assume numpy input for both mel spect and embedding\n",
    "    mel, len_pad = pad_seq(mel)\n",
    "    \n",
    "    mel       = torch.from_numpy(      mel[np.newaxis, ...]).to(device)\n",
    "    embed_in  = torch.from_numpy( embed_in[np.newaxis, ...]).to(device)\n",
    "    embed_out = torch.from_numpy(embed_out[np.newaxis, ...]).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        mel_no_PN, mel_yes_PN, _ = auto_VC(mel, embed_in, embed_out)\n",
    "            \n",
    "        if len_pad == 0:\n",
    "            mel_no_PN  =  mel_no_PN[0, 0, :, :].cpu().numpy()\n",
    "            mel_yes_PN = mel_yes_PN[0, 0, :, :].cpu().numpy()\n",
    "        else:\n",
    "            mel_no_PN  =  mel_no_PN[0, 0, :-len_pad, :].cpu().numpy()\n",
    "            mel_yes_PN = mel_yes_PN[0, 0, :-len_pad, :].cpu().numpy()\n",
    "    \n",
    "    return mel_no_PN, mel_yes_PN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified from https://github.com/miaoYuanyuan/gen_melSpec_from_wav\n",
    "sr = 16000\n",
    "n_fft = 1024\n",
    "win_length = 1024\n",
    "hop_length = 256\n",
    "n_mels = 80\n",
    "fmin = 90\n",
    "fmax = 7600\n",
    "ref_level_db = 16\n",
    "min_level_db = -100\n",
    "\n",
    "# def visualize_spect(spect, title=None):\n",
    "#     plt.figure()\n",
    "#     if title is not None:\n",
    "#         plt.title(title)\n",
    "#     plt.imshow(np.flip(spect**.25, 0))\n",
    "#     plt.show()\n",
    "    \n",
    "def _amp_to_db(x):\n",
    "    min_level = np.exp(min_level_db / 20 * np.log(10))\n",
    "    return 20 * np.log10(np.maximum(min_level, x))\n",
    "\n",
    "def _db_to_amp(x):\n",
    "    return np.power(10.0, x * 0.05)\n",
    "    \n",
    "def normalize_for_VC(mel):\n",
    "    # assume magnitude melspectrum with correct sr/fmin/fmax as input\n",
    "    mel = _amp_to_db(mel) - ref_level_db\n",
    "    mel = np.clip((mel - min_level_db) / -min_level_db, 0, 1)\n",
    "    return mel.T\n",
    "\n",
    "def denormalize_from_VC(mel):\n",
    "    mel = (np.clip(mel, 0, 1) * -min_level_db) + min_level_db\n",
    "    mel = _db_to_amp(mel + ref_level_db)\n",
    "    return mel.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_experiment(\n",
    "    audio_in, \n",
    "    lin_to_mel = True, \n",
    "    mel_to_lin = False, \n",
    "    apply_VC   = True, \n",
    "    embed_in   = None,\n",
    "    embed_out  = None,\n",
    "    apply_PN   = True, \n",
    "    inversion  = \"WV\" # or \"GL\"\n",
    "):    \n",
    "    lin_in = np.abs(librosa.stft(audio_in, n_fft=n_fft, win_length=win_length, hop_length=hop_length))\n",
    "    \n",
    "    if lin_to_mel:\n",
    "        mel_in = librosa.feature.melspectrogram(S=lin_in, sr=sr, n_fft=n_fft, fmin=fmin, fmax=fmax, n_mels=n_mels) \n",
    "        \n",
    "    if apply_VC:\n",
    "        mel_in = normalize_for_VC(mel_in)\n",
    "        \n",
    "        mel_no_PN, mel_yes_PN = apply_autoVC(mel_in, embed_in, embed_out)        \n",
    "        mel_out = mel_yes_PN if apply_PN else mel_no_PN \n",
    "    else:\n",
    "        lin_out = lin_in\n",
    "        mel_out = mel_in\n",
    "    \n",
    "    if inversion == \"WV\":\n",
    "        if not apply_VC:\n",
    "            mel_out = normalize_for_VC(mel_out)\n",
    "        audio = wavegen(wavenet_vocoder, c=mel_out)\n",
    "    \n",
    "    if mel_to_lin:\n",
    "        if apply_VC:\n",
    "            mel_out = denormalize_from_VC(mel_out)\n",
    "        lin_out = librosa.feature.inverse.mel_to_stft(mel_out, n_fft=n_fft, sr=sr, fmin=fmin, fmax=fmax) \n",
    "    \n",
    "    if inversion == \"GL\":\n",
    "        audio = librosa.griffinlim(lin_out, win_length=win_length, hop_length=hop_length)\n",
    "        \n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example experiment\n",
    "audio_in, _ = librosa.load('audio/p225_001.wav', sr)\n",
    "\n",
    "embed_in = spkr_to_embed['p225']\n",
    "embed_out = spkr_to_embed['p256']\n",
    "\n",
    "audio_out = perform_experiment(audio_in, embed_in=embed_in, embed_out=embed_out)\n",
    "\n",
    "ipd.display(ipd.Audio(audio_in, rate=sr))\n",
    "ipd.display(ipd.Audio(audio_out, rate=sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all experiments in a loop (only convert between p225 and p256)\n",
    "\n",
    "def save_and_show(audio, path):\n",
    "    print(path)\n",
    "    librosa.output.write_wav(path=path, y=audio, sr=sr)\n",
    "    ipd.display(ipd.Audio(audio, rate=sr))\n",
    "\n",
    "for fname in ['audio/p225_001.wav', 'audio/p256_002.wav']:\n",
    "    spkr_in = fname.split(\"/\")[-1][:4]\n",
    "    for spkr_out in ['p225', 'p256']:\n",
    "        audio_in, _ = librosa.load(fname, sr=sr)\n",
    "        \n",
    "        embed_in  = spkr_to_embed[spkr_in]\n",
    "        embed_out = spkr_to_embed[spkr_out]\n",
    "        \n",
    "        # A\n",
    "        audio_out = perform_experiment(\n",
    "            audio_in,\n",
    "            lin_to_mel = True,\n",
    "            apply_VC   = False,\n",
    "            apply_PN   = False,\n",
    "            inversion  = \"WV\"\n",
    "        )\n",
    "        path = 'audio/'+spkr_in+'_A_lin_mel_WV.wav'\n",
    "        save_and_show(audio_out, path)\n",
    "                \n",
    "        # B\n",
    "        audio_out = perform_experiment(\n",
    "            audio_in,\n",
    "            apply_VC   = False,\n",
    "            apply_PN   = False,\n",
    "            inversion  = \"GL\"\n",
    "        )\n",
    "        path = 'audio/'+spkr_in+'_B_lin_GL.wav'\n",
    "        save_and_show(audio_out, path)\n",
    "        \n",
    "        # C\n",
    "        audio_out = perform_experiment(\n",
    "            audio_in,\n",
    "            lin_to_mel = True,\n",
    "            mel_to_lin = True,\n",
    "            apply_VC   = False,\n",
    "            apply_PN   = False,\n",
    "            inversion  = \"GL\"\n",
    "        )\n",
    "        path = 'audio/'+spkr_in+'_C_lin_mel_lin_GL.wav'\n",
    "        save_and_show(audio_out, path)\n",
    "        \n",
    "        # D\n",
    "        audio_out = perform_experiment(\n",
    "            audio_in,\n",
    "            lin_to_mel = True,\n",
    "            apply_VC   = True,\n",
    "            embed_in   = embed_in,\n",
    "            embed_out  = embed_out,\n",
    "            apply_PN   = True,\n",
    "            inversion  = \"WV\"\n",
    "        )\n",
    "        path = 'audio/'+spkr_in+'_D_'+spkr_out+'_lin_mel_VC_mel_PN_mel_WV.wav'\n",
    "        save_and_show(audio_out, path)\n",
    "        \n",
    "        # E\n",
    "        audio_out = perform_experiment(\n",
    "            audio_in,\n",
    "            lin_to_mel = True,\n",
    "            apply_VC   = True,\n",
    "            embed_in   = embed_in,\n",
    "            embed_out  = embed_out,\n",
    "            apply_PN   = False,\n",
    "            inversion  = \"WV\"\n",
    "        )\n",
    "        path = 'audio/'+spkr_in+'_E_'+spkr_out+'_lin_mel_VC_mel_WV.wav'\n",
    "        save_and_show(audio_out, path)\n",
    "        \n",
    "        # F\n",
    "        audio_out = perform_experiment(\n",
    "            audio_in,\n",
    "            lin_to_mel = True,\n",
    "            apply_VC   = True,\n",
    "            embed_in   = embed_in,\n",
    "            embed_out  = embed_out,\n",
    "            apply_PN   = True,\n",
    "            mel_to_lin = True,\n",
    "            inversion  = \"GL\"\n",
    "        )\n",
    "        path = 'audio/'+spkr_in+'_F_'+spkr_out+'_lin_mel_VC_mel_PN_mel_lin_GL.wav'\n",
    "        save_and_show(audio_out, path)\n",
    "        \n",
    "        # G\n",
    "        audio_out = perform_experiment(\n",
    "            audio_in,\n",
    "            lin_to_mel = True,\n",
    "            apply_VC   = True,\n",
    "            embed_in   = embed_in,\n",
    "            embed_out  = embed_out,\n",
    "            apply_PN   = False,\n",
    "            mel_to_lin = True,\n",
    "            inversion  = \"GL\"\n",
    "        )\n",
    "        path = 'audio/'+spkr_in+'_G_'+spkr_out+'_lin_mel_VC_mel_lin_GL.wav'\n",
    "        save_and_show(audio_out, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "AutoVC sounds best with WaveNet Vocoder. However, this seems to be due to the fact that AutoVC outputs mel-spectrograms. Mel-spectrograms need to be converted to linear spectrograms before they can be inverted using the Griffin-Lim algorithm, and the mel-to-linear step degrades quality. \n",
    "\n",
    "- If you have mel spectrograms, use a wavenet vocoder\n",
    "- If you have linear spectrograms, you can use the faster Griffin-Lim algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
